{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d4fae4",
   "metadata": {},
   "source": [
    "# Step 0: Setup — Imports, API key, and CSV loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5fe762",
   "metadata": {},
   "source": [
    "### Step 0.1: Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da43a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20vli\\anaconda3\\envs\\arp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# API Libraries\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM Libraries\n",
    "import requests\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a153b5",
   "metadata": {},
   "source": [
    "### Step 0.2: Loading API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e7499d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read API keys for both providers\n",
    "DEEPSEEK_KEY = os.getenv(\"DEEPSEEK_API_KEY\")   # DeepSeek API key\n",
    "GEMINI_KEY   = os.getenv(\"GEMINI_API_KEY\")     # Gemini API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fea7a4",
   "metadata": {},
   "source": [
    "### Step 0.3: Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b82840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onboarding     : 4 rows, 48 columns\n",
      "Sarah training : 90 rows, 3 columns\n",
      "Leo training   : 90 rows, 3 columns\n",
      "Urja training  : 90 rows, 3 columns\n"
     ]
    }
   ],
   "source": [
    "# Load persona onboarding data\n",
    "onboarding_df = pd.read_csv(\"../data/Onboarding_QnA.csv\")\n",
    "\n",
    "# Load training Q&A datasets for each persona\n",
    "sarah_train = pd.read_csv(\"../data/Sarah_QnA.csv\")\n",
    "leo_train   = pd.read_csv(\"../data/Leo_QnA.csv\")\n",
    "urja_train  = pd.read_csv(\"../data/Urja_QnA.csv\")\n",
    "\n",
    "# Print a quick check on all dataset shapes\n",
    "datasets = {\n",
    "    \"Onboarding\": onboarding_df,\n",
    "    \"Sarah training\": sarah_train,\n",
    "    \"Leo training\": leo_train,\n",
    "    \"Urja training\": urja_train\n",
    "}\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name:15}: {len(df)} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d3ca0d",
   "metadata": {},
   "source": [
    "# Step 1: Helper Functions (re-used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c37a5",
   "metadata": {},
   "source": [
    "The functions: `get_persona_row()`, `build_persona_text()`, `build_examples_blob()`, and `trim_to_char_budget()` are directly reused from the `llm_evaluation.ipynb` code file without modification. They handle persona loading, training Q&A formatting, and trimming prompts to character limit. For detailed inline explanation please refer to the evalution script Sections 1 & 2 (in-line comments are referecnes to sections the functions are from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d23e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Fetching a persona’s onboarding data by name\n",
    "def get_persona_row(name: str) -> pd.Series:\n",
    "    mask = onboarding_df[\"Persona\"].astype(str).str.strip().str.casefold() == name.strip().casefold()\n",
    "    if not mask.any():\n",
    "        raise ValueError(f\"Persona '{name}' not found in onboarding_df['Persona']\")\n",
    "    return onboarding_df.loc[mask].iloc[0]\n",
    "\n",
    "# Step 1.2: Building a persona text block from onboarding data\n",
    "def build_persona_text(row: pd.Series) -> str:\n",
    "    lines = []\n",
    "    for col in onboarding_df.columns:\n",
    "        val = str(row.get(col, \"\")).strip()\n",
    "        if val and val.lower() != \"nan\":\n",
    "            lines.append(f\"{col}: {val}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Step 2.1: Building an examples blob for a specific setting\n",
    "def build_examples_blob(train_df: pd.DataFrame, setting: str) -> str:\n",
    "    sub = train_df[\n",
    "        train_df[\"Setting\"].astype(str).str.strip().str.casefold() == setting.strip().casefold()\n",
    "    ].dropna(subset=[\"Question\", \"Answer\"])\n",
    "    lines = [f\"- Q: {q}\\n  A: {a}\" for q, a in zip(sub[\"Question\"], sub[\"Answer\"])]\n",
    "    return f\"Examples of past answers (Setting: {setting}, count={len(lines)}):\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "# Step 2.2 : Trimming text to fit within character budget\n",
    "def trim_to_char_budget(text: str, max_chars: int = 60000) -> str:\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    out, total = [], 0\n",
    "    for ln in text.splitlines():\n",
    "        L = len(ln) + 1\n",
    "        if total + L > max_chars:\n",
    "            break\n",
    "        out.append(ln)\n",
    "        total += L\n",
    "    out.append(\"\\n...[trimmed due to context budget]...\")\n",
    "    return \"\\n\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e97712",
   "metadata": {},
   "source": [
    "# Step 2: Helper Functions (prototype-specifc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793e7b1",
   "metadata": {},
   "source": [
    "### Step 2.1: Allowed values for settings and providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233deb25",
   "metadata": {},
   "source": [
    "Define fixed sets of valid settings and LLM providers. These are used for validation so the prototype only accepts supported options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "adfcb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_SETTINGS = {\"Family & Friends\", \"Medical\", \"Work\"}\n",
    "ALLOWED_PROVIDERS = {\"deepseek\", \"gemini\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2464a",
   "metadata": {},
   "source": [
    "### Step 2.2: Map persona name to their training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ee43a",
   "metadata": {},
   "source": [
    "The function `get_train_df_for_persona()` selects the correct trainign Q&A df based on persona name. This helps to minimize the number of variables to pass to final prototype function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af4690b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_df_for_persona(persona_name: str) -> pd.DataFrame:\n",
    "\n",
    "    # Map known persona names to their corresponding training datasets\n",
    "    mapping = {\n",
    "        \"Sarah Ahmed\": sarah_train,\n",
    "        \"Leonardo Carrey\": leo_train,\n",
    "        \"Urja Mir\": urja_train,\n",
    "    }\n",
    "\n",
    "    # If the provided name isn’t in the mapping, raise a clear error\n",
    "    if persona_name not in mapping:\n",
    "        raise ValueError(f\"Unknown persona '{persona_name}'. Choose from: {list(mapping.keys())}\")\n",
    "    \n",
    "    # Return the matching training DataFrame\n",
    "    return mapping[persona_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce0a26",
   "metadata": {},
   "source": [
    "### Step 2.3: Prototype-specific initial prompt builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfeb0f",
   "metadata": {},
   "source": [
    "The function `build_full_user_prompt_proto()` constructs the primary prompt consisting of setting, relevant Q&A examples, question, sentiment, and number of responses for the LLM to use for response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e9b2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_user_prompt_proto(setting: str, question: str,\n",
    "                                 examples_blob: str, num_responses: int,\n",
    "                                 sentiment: str) -> str:\n",
    "    \n",
    "    # Ensure the examples section stays within a safe character budget\n",
    "    examples_blob = trim_to_char_budget(examples_blob, 60000)\n",
    "    \n",
    "    # Construct the initial user prompt\n",
    "    return (\n",
    "        f\"Conversation setting: {setting}\\n\\n\"\n",
    "        f\"{examples_blob}\\n\\n\"\n",
    "        f\"New question: {question}\\n\\n\"\n",
    "        f\"Use a HARD '{sentiment}' sentiment in every option.\\n\"\n",
    "        f\"Provide exactly {num_responses} concise reply option(s), labelled 1, 2, 3... (no extra text).\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de63b13",
   "metadata": {},
   "source": [
    "### Step 2.4: Flexible option splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da950e",
   "metadata": {},
   "source": [
    "The function `split_numbered_options()` parses the LLM output test into exactly *n* clear options (with limit set 1-5/10). It extend the basic splitter we used in evalution to allow for a flexible n-responses parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21ed5227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern for numbered/bulleted lines\n",
    "_num_pat = re.compile(r\"^\\s*(?:\\d+[\\).\\-:]|\\-\\s*|\\•\\s*)\\s*(.+?)\\s*$\")\n",
    "\n",
    "def split_numbered_options(text: str, n: int):\n",
    "\n",
    "    # Ensure the number of responses is within the supported range (1–10) \n",
    "    n = max(1, min(10, int(n)))  # TODO: adjust \"min([], int(n))\" for higher/lower limits\n",
    "\n",
    "    # Handle invalid or empty input early\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [\"\"] * n\n",
    "\n",
    "    # First pass: line-wise cleanup + regex extraction\n",
    "    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]\n",
    "    opts = []\n",
    "    for ln in lines:\n",
    "        m = _num_pat.match(ln)\n",
    "        opts.append(m.group(1).strip() if m else ln)\n",
    "\n",
    "    # Fallback: if too few, split by common numbering tokens\n",
    "    if len([o for o in opts if o]) < n:\n",
    "        chunks = re.split(r\"(?:^|\\s)(?:1\\.|2\\.|3\\.|4\\.|5\\.|6\\.|7\\.|8\\.|9\\.|10\\.)\\s*\", text)\n",
    "        chunks = [c.strip() for c in chunks if c.strip()]\n",
    "        merged = []\n",
    "        for x in opts + chunks:\n",
    "            if x and x not in merged:\n",
    "                merged.append(x)\n",
    "        opts = merged\n",
    "\n",
    "    # Normalise to exactly n options (truncate or pad with empties)\n",
    "    return (opts + [\"\"] * n)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7208b4",
   "metadata": {},
   "source": [
    "# Step 3: Prototype Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prototype(question: str,\n",
    "                  provider: str = \"gemini\",\n",
    "                  num_responses: int = 3,\n",
    "                  sentiment: str = \"Neutral\",\n",
    "                  persona_name: str = \"Sarah Ahmed\",\n",
    "                  setting: str = \"Family & Friends\",\n",
    "                  temperature: float = 1,\n",
    "                  gemini_model: str = \"gemini-2.5-flash\",\n",
    "                  deepseek_model: str = \"deepseek-chat\",\n",
    "                  evaluate: bool = False,               \n",
    "                  ):\n",
    "\n",
    "    # (1) Validate provider\n",
    "    provider = provider.lower().strip()\n",
    "    if provider not in ALLOWED_PROVIDERS:\n",
    "        raise ValueError(f\"provider must be one of {ALLOWED_PROVIDERS}\")\n",
    "\n",
    "    # (2) Validate setting\n",
    "    if setting not in ALLOWED_SETTINGS:\n",
    "        raise ValueError(f\"setting must be one of {ALLOWED_SETTINGS} (got '{setting}')\")\n",
    "\n",
    "    # (3) Validate number of responses\n",
    "    num_responses = int(num_responses)\n",
    "    if not (1 <= num_responses <= 10):                             # TODO: adjust \"(1 <= num_responses <= [])\" for higher/lower limits\n",
    "        raise ValueError(\"num_responses must be between 1 and 5\")  # TODO: adjust \"num_responses must be between 1 and []\" for higher/lower limits\n",
    "\n",
    "    # (4) Persona context setup\n",
    "    persona_row = get_persona_row(persona_name)             # Persna row from onboarding_df\n",
    "    persona_text = build_persona_text(persona_row)          # Formatting persona_row into text block\n",
    "    train_df = get_train_df_for_persona(persona_name)       # Load persona specific Q&A training data\n",
    "    examples_blob = build_examples_blob(train_df, setting)  # Filter examples for the given setting & persona\n",
    "\n",
    "    # (5) User prompt\n",
    "    user_prompt = build_full_user_prompt_proto(setting, question, examples_blob,\n",
    "                                               num_responses, sentiment)\n",
    "    \n",
    "    # (6) System prompt\n",
    "#### -------------------------------------------------- EDIT FOR EVALUATION -------------------------------------------------- ####\n",
    "    system_prompt = (\n",
    "            f\"Task: Generate {num_responses} responses as {persona_name}.\\n\\n\"\n",
    "            f\"Context:\\n{persona_text}\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- First person\\n\"\n",
    "            \"- <30 words\\n\"\n",
    "            f\"- {sentiment} tone\\n\"\n",
    "            \"- Format: 1) response 2) response 3) response ... n) response\\n\"\n",
    "    )\n",
    "#### ------------------------------------------------------------------------------------------------------------------------- ####\n",
    "    # (7) Single API call (branched by LLM provider)\n",
    "    # DEEPSEEK #\n",
    "    if provider == \"deepseek\":\n",
    "\n",
    "        # Ensure the DeepSeek API key is set\n",
    "        if not DEEPSEEK_KEY:\n",
    "            raise ValueError(\"❌ DEEPSEEK_API_KEY not found in .env\")\n",
    "        \n",
    "        # DeepSeek: role-based chat completion\n",
    "        url = \"https://api.deepseek.com/v1/chat/completions\"  # Endpoint for DeepSeek's chat-completion API\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_KEY}\",        # API key auth\n",
    "              \"Content-Type\": \"application/json\"              # Content type\n",
    "        }\n",
    "\n",
    "        # Prepare the payload with system and user prompts\n",
    "        payload = {\n",
    "#### -------------------------------------------------- EDIT FOR EVALUATION -------------------------------------------------- ####\n",
    "            \"model\": deepseek_model,                           # Specific DeepSeek model\n",
    "#### ------------------------------------------------------------------------------------------------------------------------- ####\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # System prompt\n",
    "                {\"role\": \"user\", \"content\": user_prompt},      # User prompt\n",
    "            ],\n",
    "            \"temperature\": temperature,  # Temperature controls randomness\n",
    "            \"max_tokens\": 6000,  # Limit response length\n",
    "        }\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=180)  # Send the POST request to DeepSeek API\n",
    "        resp.raise_for_status()                                                # Raise an error if the request failed\n",
    "        raw_text = resp.json()[\"choices\"][0][\"message\"][\"content\"]             # Extract the generated content from the response\n",
    "    \n",
    "    # GEMINI #\n",
    "    else:\n",
    "\n",
    "        # Ensure the Gemini API key is set\n",
    "        if not GEMINI_KEY:\n",
    "            raise ValueError(\"❌ GEMINI_API_KEY not found in .env\")\n",
    "        \n",
    "        # Gemini: configure client and send combined prompt\n",
    "        genai.configure(api_key=GEMINI_KEY)                                       # Configure the Gemini client with the API key\n",
    "#### -------------------------------------------------- EDIT FOR EVALUATION -------------------------------------------------- ####\n",
    "        model = genai.GenerativeModel(model_name=gemini_model)         # Select the Gemini model\n",
    "#### ------------------------------------------------------------------------------------------------------------------------- ####\n",
    "        prompt = f\"{system_prompt}\\n\\n{user_prompt}\"                              # Build the full prompt by concatenating system + user parts\n",
    "        response = model.generate_content(prompt)                                 # Send the prompt to the Gemini API\n",
    "        raw_text = response.text if hasattr(response, \"text\") else str(response)  # Extract the generated text output.\n",
    "\n",
    "    # (8) Parse results into N options\n",
    "    options = split_numbered_options(raw_text, num_responses)\n",
    "\n",
    "    # (9) Print results\n",
    "    if evaluate == False:\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Persona: {persona_name}  |  Provider: {provider}\")\n",
    "        print(f\"Sentiment: {sentiment}  |  Responses: {num_responses}  |  Setting: {setting}\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"QUESTION: '{question}'\")\n",
    "        print(\"-\" * 100)\n",
    "        for i, opt in enumerate(options, 1):\n",
    "            print(f\"{i}. {opt}\")\n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0c69f",
   "metadata": {},
   "source": [
    "# Step 4: Run the Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76e08a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Persona: Sarah Ahmed  |  Provider: deepseek\n",
      "Sentiment: positive  |  Responses: 5  |  Setting: Medical\n",
      "----------------------------------------------------------------------------------------------------\n",
      "QUESTION: 'Do you feel well-rested after sleeping last night?'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1. Alhamdulillah, I slept wonderfully last night and feel refreshed and ready for the day! 🌞\n",
      "2. Yes, I had a very restful night - feeling energized and grateful for good sleep.\n",
      "3. Absolutely! Slept like a baby and woke up feeling peaceful and well-rested. 😊\n",
      "4. Definitely well-rested - my morning prayers felt extra meaningful after such good rest.\n",
      "5. I feel fantastic! Great sleep last night has me in a wonderful mood today. YOLO!\n",
      "====================================================================================================\n",
      "______________________________\n",
      " EXECUTION TIME: 9.54 seconds\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "run_prototype(\n",
    "    question=\"Do you feel well-rested after sleeping last night?\",\n",
    "    provider=\"deepseek\",\n",
    "    num_responses=5,\n",
    "    sentiment=\"positive\",\n",
    "    persona_name=\"Sarah Ahmed\",\n",
    "    setting=\"Medical\",\n",
    "    stm_prompt=2\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "print(\"_\" * 30)\n",
    "print(f\" EXECUTION TIME: {end - start:.2f} seconds\")\n",
    "print(\"_\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab1975",
   "metadata": {},
   "source": [
    "# Step 5: Time Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2ff2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_mdl = [\"deepseek-chat\", \"deepseek-reasoner\"]\n",
    "gemini_mdl = [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\"]\n",
    "prompt = [1, 2, 3]\n",
    "number_responses = [1, 3, 5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69ed41",
   "metadata": {},
   "source": [
    "### Step 5.1: Number of responses (DeepSeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b30e3",
   "metadata": {},
   "source": [
    "Evaluating how number of respones generated effects output time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c947900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Responses =1 | AVERAGE EXECUTION TIME (10 runs): 5.89 seconds\n",
      "____________________________________________________________________________________________________\n",
      "N-Responses =3 | AVERAGE EXECUTION TIME (10 runs): 8.58 seconds\n",
      "____________________________________________________________________________________________________\n",
      "N-Responses =5 | AVERAGE EXECUTION TIME (10 runs): 9.88 seconds\n",
      "____________________________________________________________________________________________________\n",
      "N-Responses =10 | AVERAGE EXECUTION TIME (10 runs): 14.05 seconds\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in number_responses:\n",
    "    times = []\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        run_prototype(\n",
    "            question=\"Do you feel well-rested after sleeping last night?\",\n",
    "            provider=\"deepseek\",\n",
    "            num_responses=i,\n",
    "            sentiment=\"positive\",\n",
    "            persona_name=\"Sarah Ahmed\",\n",
    "            setting=\"Medical\",\n",
    "            evaluate=True\n",
    "        )\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_time = sum(times) / len(times)  # average runtime\n",
    "\n",
    "    print(f\"N-Responses ={i} | AVERAGE EXECUTION TIME (10 runs): {avg_time:.2f} seconds\")\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e253f",
   "metadata": {},
   "source": [
    "### 5.2: Prompt Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451aa2e",
   "metadata": {},
   "source": [
    "Evaluating how prompt compexity effects output time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d702d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt =1 | AVERAGE EXECUTION TIME (10 runs): 9.62 seconds\n",
      "____________________________________________________________________________________________________\n",
      "Prompt =2 | AVERAGE EXECUTION TIME (10 runs): 9.76 seconds\n",
      "____________________________________________________________________________________________________\n",
      "Prompt =3 | AVERAGE EXECUTION TIME (10 runs): 9.94 seconds\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in prompt:\n",
    "    times = []\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        run_prototype(\n",
    "            question=\"Do you feel well-rested after sleeping last night?\",\n",
    "            provider=\"deepseek\",\n",
    "            num_responses=5,\n",
    "            sentiment=\"positive\",\n",
    "            persona_name=\"Sarah Ahmed\",\n",
    "            setting=\"Medical\",\n",
    "            stm_prompt=i,\n",
    "            evaluate=True\n",
    "        )\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_time = sum(times) / len(times)  # average runtime\n",
    "\n",
    "    print(f\"Prompt ={i} | AVERAGE EXECUTION TIME (10 runs): {avg_time:.2f} seconds\")\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11347a03",
   "metadata": {},
   "source": [
    "### 5.3: DeepSeek Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172dd3f",
   "metadata": {},
   "source": [
    "Evaluating how DeepSeek model effects output time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5a91410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =deepseek-chat | AVERAGE EXECUTION TIME (10 runs): 9.56 seconds\n",
      "____________________________________________________________________________________________________\n",
      "Model =deepseek-reasoner | AVERAGE EXECUTION TIME (10 runs): 19.60 seconds\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in deepseek_mdl:\n",
    "    times = []\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        run_prototype(\n",
    "            question=\"Do you feel well-rested after sleeping last night?\",\n",
    "            provider=\"deepseek\",\n",
    "            num_responses=5,\n",
    "            sentiment=\"positive\",\n",
    "            persona_name=\"Sarah Ahmed\",\n",
    "            setting=\"Medical\",\n",
    "            deepseek_model=i,\n",
    "            evaluate=True\n",
    "        )\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_time = sum(times) / len(times)  # average runtime\n",
    "\n",
    "    print(f\"Model ={i} | AVERAGE EXECUTION TIME (10 runs): {avg_time:.2f} seconds\")\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91fc08e",
   "metadata": {},
   "source": [
    "### Step 5.4: Gemini Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb9527",
   "metadata": {},
   "source": [
    "Evaluating how Gemini model effects output time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c5b2315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =gemini-2.5-flash-lite | AVERAGE EXECUTION TIME (10 runs): 1.07 seconds\n",
      "____________________________________________________________________________________________________\n",
      "Model =gemini-2.5-flash | AVERAGE EXECUTION TIME (10 runs): 7.16 seconds\n",
      "____________________________________________________________________________________________________\n",
      "Model =gemini-2.0-flash-lite | AVERAGE EXECUTION TIME (10 runs): 1.02 seconds\n",
      "____________________________________________________________________________________________________\n",
      "Model =gemini-2.0-flash | AVERAGE EXECUTION TIME (10 runs): 1.07 seconds\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in gemini_mdl:\n",
    "    times = []\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        run_prototype(\n",
    "            question=\"Do you feel well-rested after sleeping last night?\",\n",
    "            provider=\"gemini\",\n",
    "            num_responses=5,\n",
    "            sentiment=\"positive\",\n",
    "            persona_name=\"Sarah Ahmed\",\n",
    "            setting=\"Medical\",\n",
    "            gemini_model=i,\n",
    "            evaluate=True\n",
    "        )\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_time = sum(times) / len(times)  # average runtime\n",
    "\n",
    "    print(f\"Model ={i} | AVERAGE EXECUTION TIME (10 runs): {avg_time:.2f} seconds\")\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd342c",
   "metadata": {},
   "source": [
    "### 5.5: Number of responses (Gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec3c61",
   "metadata": {},
   "source": [
    "Evaluating how number of responses generated effects output time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b222839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Responses =1 | AVERAGE EXECUTION TIME (10 runs): 7.17 seconds\n",
      "____________________________________________________________________________________________________\n",
      "N-Responses =3 | AVERAGE EXECUTION TIME (10 runs): 6.86 seconds\n",
      "____________________________________________________________________________________________________\n",
      "N-Responses =5 | AVERAGE EXECUTION TIME (10 runs): 7.77 seconds\n",
      "____________________________________________________________________________________________________\n",
      "N-Responses =10 | AVERAGE EXECUTION TIME (10 runs): 7.20 seconds\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in number_responses:\n",
    "    times = []\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        run_prototype(\n",
    "            question=\"Do you feel well-rested after sleeping last night?\",\n",
    "            provider=\"gemini\",\n",
    "            num_responses=i,\n",
    "            sentiment=\"positive\",\n",
    "            persona_name=\"Sarah Ahmed\",\n",
    "            setting=\"Medical\",\n",
    "            evaluate=True\n",
    "        )\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_time = sum(times) / len(times)  # average runtime\n",
    "\n",
    "    print(f\"N-Responses ={i} | AVERAGE EXECUTION TIME (10 runs): {avg_time:.2f} seconds\")\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952a60c",
   "metadata": {},
   "source": [
    "I am not evalating the prompt for gemini as it showed no effect with DeepSeek, therefore unlikely to have any with Gemini either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
