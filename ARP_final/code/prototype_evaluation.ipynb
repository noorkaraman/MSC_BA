{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6105d2",
   "metadata": {},
   "source": [
    "# Step 0: Setup — Imports, API key, and CSV loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98342818",
   "metadata": {},
   "source": [
    "### Step 0.1: Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1cbb34e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# API Libraries\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM Libraries\n",
    "import requests\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b55d7a",
   "metadata": {},
   "source": [
    "### Step 0.2: Loading API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98fa400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read API keys for both providers\n",
    "DEEPSEEK_KEY = os.getenv(\"DEEPSEEK_API_KEY\")   # DeepSeek API key\n",
    "GEMINI_KEY   = os.getenv(\"GEMINI_API_KEY\")     # Gemini API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8cfbf",
   "metadata": {},
   "source": [
    "### Step 0.3: Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "49c6e437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onboarding     : 4 rows, 48 columns\n",
      "Sarah training : 90 rows, 3 columns\n",
      "Leo training   : 90 rows, 3 columns\n",
      "Urja training  : 90 rows, 3 columns\n",
      "Test questions : 30 rows, 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Load persona onboarding data\n",
    "onboarding_df = pd.read_csv(\"../data/Onboarding_QnA.csv\")\n",
    "\n",
    "# Load training Q&A datasets for each persona\n",
    "sarah_train = pd.read_csv(\"../data/Sarah_QnA.csv\")\n",
    "leo_train   = pd.read_csv(\"../data/Leo_QnA.csv\")\n",
    "urja_train  = pd.read_csv(\"../data/Urja_QnA.csv\")\n",
    "\n",
    "# Load test questions to evaluate the model outputs\n",
    "test_df = pd.read_csv(\"../data/Test_Qs.csv\")\n",
    "\n",
    "# Print a quick check on all dataset shapes\n",
    "datasets = {\n",
    "    \"Onboarding\": onboarding_df,\n",
    "    \"Sarah training\": sarah_train,\n",
    "    \"Leo training\": leo_train,\n",
    "    \"Urja training\": urja_train,\n",
    "    \"Test questions\": test_df,\n",
    "}\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name:15}: {len(df)} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beafca3",
   "metadata": {},
   "source": [
    "# Step 1: Extracting and formatting persona onboarding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed2787",
   "metadata": {},
   "source": [
    "### Step 1.1: Fetching a persona’s onboarding data by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8010db1",
   "metadata": {},
   "source": [
    "The function `get_persona_row()` looks up persona in the onboarding dataset by name and returns the row for that persona to use in Step 1.2. It also ensures the persona's existance in the file.\n",
    "\n",
    "*P.S. Error checks like this are common through out the file, it helps us to know exatly where to look when the code fails without the need for debugging.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e109794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_persona_row(name: str) -> pd.Series:\n",
    "    \n",
    "    # Create a boolean mask where \"Persona\" column matches the provided name (ensures case insensitivity and stripping whitespace)\n",
    "    mask = onboarding_df[\"Persona\"].astype(str).str.strip().str.casefold() == name.strip().casefold()\n",
    "    \n",
    "    # Raise error if persona not found\n",
    "    if not mask.any():\n",
    "        raise ValueError(f\"Persona '{name}' not found in onboarding_df['Persona']\")\n",
    "    \n",
    "    # Return the first matching row (when used in a product there will only be one name in the dataset, but we test diffent personas here)\n",
    "    return onboarding_df.loc[mask].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271545e6",
   "metadata": {},
   "source": [
    "### Step 1.2: Building a persona text block from onboarding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb1e56",
   "metadata": {},
   "source": [
    "The fuction `build_persona_text()` takes a single persona's onboarding info and converts it into a structured text block. This provides a clear persona description for LLM to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c55ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_persona_text(row: pd.Series) -> str:\n",
    "    \n",
    "    # Collect formatted \"column: value\" pairs\n",
    "    lines = []\n",
    "\n",
    "    # Loop over every column in the onboarding dataset\n",
    "    for col in onboarding_df.columns:\n",
    "        val = str(row.get(col, \"\")).strip()  # Get the value and clean whitespace\n",
    "\n",
    "        # Only keep non-empty and non-NaN values\n",
    "        if val and val.lower() != \"nan\":\n",
    "            lines.append(f\"{col}: {val}\")    # Format as \"ColumnName: Value\"\n",
    "\n",
    "    # Join all entries into a multi-line string\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc31519",
   "metadata": {},
   "source": [
    "# Step 2: Preparing contextual examples and constructing user prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd698cb",
   "metadata": {},
   "source": [
    "### Step 2.1: Building an examples blob for a specific setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e04ce",
   "metadata": {},
   "source": [
    "The function `build_example_blob()` filters the training Q&A from the files for a given persona by the setting. Then it formats the pairs of Q&A into examples blocks for the LLM to use as training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82232082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_examples_blob(train_df: pd.DataFrame, setting: str) -> str:\n",
    "    \n",
    "    # Filter the data to get only the rows matching the specified setting (droppign all else)\n",
    "    sub = train_df[\n",
    "        train_df[\"Setting\"].astype(str).str.strip().str.casefold() == setting.strip().casefold()\n",
    "    ].dropna(subset=[\"Question\", \"Answer\"])\n",
    "\n",
    "    # Format each Q&A pair into a string as \"- Q: <question>\\n  A: <answer>\"\n",
    "    lines = [f\"- Q: {q}\\n  A: {a}\" for q, a in zip(sub[\"Question\"], sub[\"Answer\"])]\n",
    "\n",
    "    # Join all pairs into a single string\n",
    "    return f\"Examples of past answers (Setting: {setting}, count={len(lines)}):\\n\" + \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f28219",
   "metadata": {},
   "source": [
    "### Step 2.2 : Trimming text to fit within character budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7761f1",
   "metadata": {},
   "source": [
    "The function `trim_to_char_budget()` ensures that long text does not exceed the limit. Helps to keep prompts within the LLM's context window. It is not applicable to us, however, a larger training/ onboarding set or prompt could use this function with a specified character limit (i.e. future proofing function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a7d02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_to_char_budget(text: str, max_chars: int = 60000) -> str:\n",
    "\n",
    "    # If the text is already within the limit, return it as is\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    \n",
    "    # Buffer for kept lines + current total character count\n",
    "    out, total = [], 0\n",
    "\n",
    "    # Split the text into lines and iterate through them\n",
    "    for ln in text.splitlines():\n",
    "        \n",
    "        # Line length (+1 for newline char)\n",
    "        L = len(ln) + 1\n",
    "\n",
    "        # If adding line exceeds the limit, stop adding more lines\n",
    "        if total + L > max_chars:\n",
    "            break\n",
    "        out.append(ln)\n",
    "        total += L\n",
    "\n",
    "    # If the output is empty, return a placeholder message to indicate truncation\n",
    "    out.append(\"\\n...[trimmed due to context budget]...\")\n",
    "    return \"\\n\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a0b85",
   "metadata": {},
   "source": [
    "### Step 2.3: Composing the per-question user prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265917b1",
   "metadata": {},
   "source": [
    "The function `build_full_user_prompt()` assembles the final prompt by combining the conversation setting, relevant example (train) blob and clear instructions to return exactly 3 option. 3 options are used purely for evalution purposes, the prototype allows for a selection for number of options (e.g. 1-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b34e1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_user_prompt(setting: str, question: str, examples_blob: str) -> str:\n",
    "\n",
    "    # Ensure the examples section stays within a safe character budget\n",
    "    examples_blob = trim_to_char_budget(examples_blob, 60000)\n",
    "\n",
    "    # Construct the full user prompt with the conversation setting, examples, and new question\n",
    "    return (\n",
    "        f\"Conversation setting: {setting}\\n\\n\"\n",
    "        f\"{examples_blob}\\n\\n\"\n",
    "        f\"Now answer the new question in the user's voice and preferences.\\n\"\n",
    "        f\"New question: {question}\\n\\n\"\n",
    "        \"Provide exactly 3 concise reply options, labelled 1, 2, 3.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17ace1",
   "metadata": {},
   "source": [
    "# Step 3: Extracting and cleaning numbered reply options from model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652a0d9",
   "metadata": {},
   "source": [
    "The function `split_numbered_options()` defines regex pattern to recognise common numbering styles and uses it to split model outputs into 3 clean options. \n",
    "\n",
    "*re Documentation: [LINK](https://docs.python.org/3/library/re.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687b579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to capture \"numbered\" or bullet-style options\n",
    "_num_pat = re.compile(r\"^\\s*(?:\\d+[\\).\\-:]|\\-\\s*|\\•\\s*)\\s*(.+?)\\s*$\")\n",
    "\n",
    "def split_numbered_options(text: str):\n",
    "\n",
    "    # Handle invalid or empty input\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [\"\", \"\", \"\"]\n",
    "    \n",
    "    # Split the text into lines and clean them\n",
    "    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]\n",
    "\n",
    "    # Apply regex to extract options, falling back to raw lines if no match\n",
    "    opts = [(_num_pat.match(ln).group(1).strip() if _num_pat.match(ln) else ln) for ln in lines]\n",
    "\n",
    "    # If we have fewer than 3 options, try to split by common numbering patterns\n",
    "    if len(opts) < 3:\n",
    "        chunks = re.split(r\"(?:^|\\s)(?:1\\.|2\\.|3\\.)\\s*\", text)\n",
    "        chunks = [c.strip() for c in chunks if c.strip()]\n",
    "        if len(chunks) == 3:\n",
    "            opts = chunks\n",
    "\n",
    "    # Return exactly 3 options, filling with empty strings if needed\n",
    "    return (opts + [\"\", \"\", \"\"])[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e91e4",
   "metadata": {},
   "source": [
    "# Step 4: Running batched persona tests with DeepSeek or Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05afb904",
   "metadata": {},
   "source": [
    "### Step 4.1: Runner for the LLM calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951b07e",
   "metadata": {},
   "source": [
    "The function `run_batched_test_for_persona()` generates prompts for all 30 test questions, sends them in a single batched request to chosen LLM, parses the rerurned options, and saves the results into a an excel file. This is the core function of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac812d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batched_test_for_persona(persona_text: str,                       # Chosen persona text\n",
    "                                 train_df: pd.DataFrame,                  # Training data for the persona\n",
    "                                 test_df: pd.DataFrame,                   # Test questions to evaluate the model outputs\n",
    "                                 out_path: str,                           # Output path for the results\n",
    "                                 provider: str = \"deepseek\",              # LLM provider to use (\"deepseek\" or \"gemini\")\n",
    "                                 temperature: float = 1,                  # Temperature for DeepSeek defaults to 1\n",
    "                                 gemini_model: str = \"gemini-2.5-flash\",  # Gemini model to use\n",
    "                                 deepseek_model: str = \"deepseek-chat\",   # DeepSeek model to use\n",
    "                                 persona_name: str = \"Sarah Ahmed\",       # Persona name for logging\n",
    "                                 ):     \n",
    "\n",
    "    # (1) Normalize the provider input and validate\n",
    "    provider = provider.lower().strip()\n",
    "    if provider not in {\"deepseek\", \"gemini\"}:\n",
    "        raise ValueError(\"provider must be 'deepseek' or 'gemini'\")\n",
    "\n",
    "\n",
    "    # (2) Build example blobs per setting from the training data\n",
    "    example_blobs = {\n",
    "        st: build_examples_blob(train_df, st)\n",
    "        for st in sorted(test_df[\"Setting\"].astype(str).str.strip().unique())\n",
    "    }\n",
    "\n",
    "\n",
    "    # (3) Construct blocks for each test question with QID (Question ID) headers\n",
    "    sections_meta, user_blocks = [], []\n",
    "    for i, r in test_df.reset_index(drop=True).iterrows():\n",
    "        setting = str(r[\"Setting\"]).strip()                                           # Setting\n",
    "        question = str(r[\"Question\"]).strip()                                         # Question\n",
    "        blob = example_blobs.get(setting, \"\")                                         # Example blob\n",
    "        per_q_block = build_full_user_prompt(setting, question, blob)                 # Build the user prompt block\n",
    "        header = f\"### QID {i+1} | SETTING: {setting}\\n\"                              # QID Header for the question block\n",
    "        user_blocks.append(header + per_q_block)                                      # Append the full block to the user blocks\n",
    "        sections_meta.append({\"qid\": i+1, \"Setting\": setting, \"Question\": question})  # Store data for each question\n",
    "\n",
    "    # (4) Wrapper instructions: enforce structure across all QIDs\n",
    "    wrapper = (\n",
    "        \"You will receive multiple question blocks, each preceded by a header:\\n\"\n",
    "        \"### QID n | SETTING: <name>\\n\\n\"\n",
    "        \"For EVERY QID, generate exactly 3 numbered options in the user's voice.\\n\"\n",
    "        \"Reply with 30 sections in order (QID 1..30), each section starting with the SAME header, \"\n",
    "        \"followed by the three options below it.\\n\"\n",
    "    )\n",
    "\n",
    "    # Add the wrapper to the blocks, ensures the LLM knows to expect multiple blocks with headers\n",
    "    combined_user_prompt = wrapper + \"\\n\\n\".join(user_blocks)\n",
    "\n",
    "    # (5) System prompt\n",
    "#### -------------------------------------------------- EDIT FOR EVALUATION -------------------------------------------------- ####\n",
    "    system_prompt = (\n",
    "            f\"Task: Generate 3 responses as {persona_name}.\\n\\n\"\n",
    "            f\"Context:\\n{persona_text}\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- First person\\n\"\n",
    "            \"- <30 words\\n\"\n",
    "            \"- Neutral tone\\n\"\n",
    "            \"- Format: 1) response 2) response 3) response\\n\"\n",
    "    )\n",
    "#### ------------------------------------------------------------------------------------------------------------------------- ####\n",
    "    # (6) Single API call (branched by LLM provider)\n",
    "    # DEEPSEEK #\n",
    "    if provider == \"deepseek\":\n",
    "\n",
    "        # Ensure the DeepSeek API key is set\n",
    "        if not DEEPSEEK_KEY:\n",
    "            raise ValueError(\"❌ DEEPSEEK_API_KEY not found in .env\")\n",
    "\n",
    "        # DeepSeek: role-based chat completion\n",
    "        url = \"https://api.deepseek.com/v1/chat/completions\"  # Endpoint for DeepSeek's chat-completion API\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_KEY}\",        # API key auth\n",
    "            \"Content-Type\": \"application/json\"                # Content type\n",
    "        }\n",
    "\n",
    "        # Prepare the payload with system and user prompts\n",
    "        payload = {\n",
    "#### -------------------------------------------------- EDIT FOR EVALUATION -------------------------------------------------- ####\n",
    "            \"model\": deepseek_model,                               # Specific DeepSeek model name\n",
    "#### ------------------------------------------------------------------------------------------------------------------------- ####\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},      # System prompt\n",
    "                {\"role\": \"user\", \"content\": combined_user_prompt}  # User prompt (all questions)\n",
    "            ],\n",
    "            \"temperature\": temperature,  # Temperature controls randomness\n",
    "            \"max_tokens\": 6000           # Limit response length\n",
    "        }\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=180)  # Send the POST request to DeepSeek API\n",
    "        resp.raise_for_status()                                                # Raise an error if the request failed\n",
    "        big_text = resp.json()[\"choices\"][0][\"message\"][\"content\"]             # Extract the generated content from the response\n",
    "\n",
    "    # GEMINI #\n",
    "    else:\n",
    "\n",
    "        # Ensure the Gemini API key is set\n",
    "        if not GEMINI_KEY:\n",
    "            raise ValueError(\"❌ GEMINI_API_KEY not found in .env\")\n",
    "\n",
    "        # Gemini: configure client and send combined prompt\n",
    "        genai.configure(api_key=GEMINI_KEY)                                       # Configure the Gemini client with the API key\n",
    "#### -------------------------------------------------- EDIT FOR EVALUATION -------------------------------------------------- ####\n",
    "        model = genai.GenerativeModel(model_name=gemini_model)                    # Select the Gemini model\n",
    "#### ------------------------------------------------------------------------------------------------------------------------- ####\n",
    "        prompt = f\"{system_prompt}\\n\\n{combined_user_prompt}\"                     # Build the full prompt by concatenating system + user parts\n",
    "        response = model.generate_content(prompt)                                 # Send the prompt to the Gemini API\n",
    "        big_text = response.text if hasattr(response, \"text\") else str(response)  # Extract the generated text output.\n",
    "\n",
    "\n",
    "    # (7) Parse response by QID headers → map answers back to test questions\n",
    "    # Split the full response into blocks using the exact header pattern\n",
    "    parts = re.split(r\"^###\\s*QID\\s+(\\d+)\\s*\\|\\s*SETTING:\\s*(.+)$\", big_text, flags=re.M)\n",
    "    parsed_rows = []\n",
    "\n",
    "    # We expect at least one triplet (qid, setting, section_text), requires length >= 4.\n",
    "    if len(parts) >= 4:\n",
    "        # Iterate the list in steps of 3 (1 = qid, 2 = setting, 3 = section_text)\n",
    "        for idx in range(1, len(parts), 3):\n",
    "            try:\n",
    "                qid = int(parts[idx])                # Captured QID number\n",
    "                section_text = parts[idx+2].strip()  # Section text\n",
    "            \n",
    "            # If anything goes wrong, skip block\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "            # Ensure the QID is within range of our constructed metadata\n",
    "            if 1 <= qid <= len(sections_meta):\n",
    "                meta = sections_meta[qid-1]                        # Map QID back to the original test question\n",
    "                o1, o2, o3 = split_numbered_options(section_text)  # Extract exactly three numbered options from the section text\n",
    "                \n",
    "                # Append a structured row for saving later\n",
    "                parsed_rows.append({\n",
    "                    \"Setting\": meta[\"Setting\"],\n",
    "                    \"Question\": meta[\"Question\"],\n",
    "                    \"Option_1\": o1,\n",
    "                    \"Option_2\": o2,\n",
    "                    \"Option_3\": o3\n",
    "                })\n",
    "\n",
    "\n",
    "    # (8) Fallback parsing: if headers not respected (e.g. LLM ignored formatting instructions)\n",
    "    # We attempt a simpler backup strategy, split the response into paragraphs\n",
    "    if not parsed_rows:\n",
    "        chunks = re.split(r\"\\n\\s*\\n+\", big_text.strip())  # Split the text whenever there's a blank line\n",
    "\n",
    "        # Iterate over each test question in order\n",
    "        for i, meta in enumerate(sections_meta):\n",
    "            raw = chunks[i] if i < len(chunks) else big_text  # If fewer chunks than questions, reuse the whole output as the \"raw\" block\n",
    "            o1, o2, o3 = split_numbered_options(raw)          # Extract up to 3 numbered options from the chunk (using regex-based cleaner)\n",
    "\n",
    "            # Append a structured row for saving later (even if imperfect)\n",
    "            parsed_rows.append({\n",
    "                \"Setting\": meta[\"Setting\"],\n",
    "                \"Question\": meta[\"Question\"],\n",
    "                \"Option_1\": o1,\n",
    "                \"Option_2\": o2,\n",
    "                \"Option_3\": o3\n",
    "            })\n",
    "\n",
    "    # (9) Save results \n",
    "    base = out_path[:-5]   # strip \".xlsx\"\n",
    "    out_path = f\"{base}_{provider}.xlsx\"  # Auto-suffix filename by provider\n",
    "\n",
    "    # Create a DataFrame from the parsed rows and save to Excel\n",
    "    out_df = pd.DataFrame(parsed_rows)\n",
    "    out_df.to_excel(out_path, index=False)\n",
    "    print(f\"[+_+] Saved batched 30 responses: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1b939",
   "metadata": {},
   "source": [
    "### Step 4.2: Initialising personas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17b8a5",
   "metadata": {},
   "source": [
    "The block below retrieves each persona's onboarding data (row) and buids the formatted persona text using for prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100d5414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarah Q&A: 90 rows loaded.\n",
      "Leo Q&A: 90 rows loaded.\n",
      "Urja Q&A: 90 rows loaded.\n"
     ]
    }
   ],
   "source": [
    "sarah_persona_text = build_persona_text(get_persona_row(\"Sarah Ahmed\"))\n",
    "leo_persona_text   = build_persona_text(get_persona_row(\"Leonardo Carrey\"))\n",
    "urja_persona_text  = build_persona_text(get_persona_row(\"Urja Mir\"))\n",
    "\n",
    "for name, df in [(\"Sarah\", sarah_train), (\"Leo\", leo_train), (\"Urja\", urja_train)]:\n",
    "    print(f\"{name} Q&A: {len(df)} rows loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7624e",
   "metadata": {},
   "source": [
    "# Step 5: Run batched evaluation for each persona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd1103",
   "metadata": {},
   "source": [
    "### Step 5.1: DeepSeek Batched Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dac8c4",
   "metadata": {},
   "source": [
    "3 block below, run batched evaluation using DeepSeek (for each persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd2575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+_+] Saved batched 30 responses: test_responses/Sarah_Test_Responses_deepseek.xlsx\n"
     ]
    }
   ],
   "source": [
    "# SARAH (DeepSeek)\n",
    "run_batched_test_for_persona(\n",
    "    persona_text=sarah_persona_text,\n",
    "    train_df=sarah_train,\n",
    "    test_df=test_df,\n",
    "    out_path=\"test_responses/Sarah_Test_Responses.xlsx\",\n",
    "    provider=\"deepseek\",\n",
    "    persona_name=\"Sarah Ahmed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e581d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+_+] Saved batched 30 responses: test_responses/Leo_Test_Responses_deepseek.xlsx\n"
     ]
    }
   ],
   "source": [
    "# LEO (DeepSeek)\n",
    "run_batched_test_for_persona(\n",
    "    persona_text=leo_persona_text,\n",
    "    train_df=leo_train,\n",
    "    test_df=test_df,\n",
    "    out_path=\"test_responses/Leo_Test_Responses.xlsx\",\n",
    "    provider=\"deepseek\",\n",
    "    persona_name=\"Leonardo Carrey\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+_+] Saved batched 30 responses: test_responses/Urja_Test_Responses_deepseek.xlsx\n"
     ]
    }
   ],
   "source": [
    "# URJA (DeepSeek)\n",
    "run_batched_test_for_persona(\n",
    "    persona_text=urja_persona_text,\n",
    "    train_df=urja_train,\n",
    "    test_df=test_df,\n",
    "    out_path=\"test_responses/Urja_Test_Responses.xlsx\",\n",
    "    provider=\"deepseek\",\n",
    "    persona_name=\"Urja Mir\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c45bdd",
   "metadata": {},
   "source": [
    "### Step 5.2: Gemini Batched Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b60ded",
   "metadata": {},
   "source": [
    "3 block below, run batched evaluation using Gemini (for each persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+_+] Saved batched 30 responses: test_responses/Sarah_Test_Responses_gemini.xlsx\n"
     ]
    }
   ],
   "source": [
    "# SARAH (Gemini)\n",
    "run_batched_test_for_persona(\n",
    "    persona_text=sarah_persona_text,\n",
    "    train_df=sarah_train,\n",
    "    test_df=test_df,\n",
    "    out_path=\"test_responses/Sarah_Test_Responses.xlsx\",\n",
    "    provider=\"gemini\",\n",
    "    persona_name=\"Sarah Ahmed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd26297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+_+] Saved batched 30 responses: test_responses/Leo_Test_Responses_gemini.xlsx\n"
     ]
    }
   ],
   "source": [
    "# LEO (Gemini)\n",
    "run_batched_test_for_persona(\n",
    "    persona_text=leo_persona_text,\n",
    "    train_df=leo_train,\n",
    "    test_df=test_df,\n",
    "    out_path=\"test_responses/Leo_Test_Responses.xlsx\",\n",
    "    provider=\"gemini\",\n",
    "    persona_name=\"Leonardo Carrey\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5bb8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+_+] Saved batched 30 responses: test_responses/Urja_Test_Responses_gemini.xlsx\n"
     ]
    }
   ],
   "source": [
    "# URJA (Gemini)\n",
    "run_batched_test_for_persona(\n",
    "    persona_text=urja_persona_text,\n",
    "    train_df=urja_train,\n",
    "    test_df=test_df,\n",
    "    out_path=\"test_responses/Urja_Test_Responses.xlsx\",\n",
    "    provider=\"gemini\",\n",
    "    persona_name=\"Urja Mir\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b1de1",
   "metadata": {},
   "source": [
    "# Step 6: Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba275117",
   "metadata": {},
   "source": [
    "### 6.1: DeepSeek Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32999acd",
   "metadata": {},
   "source": [
    "Testing the differnt DeepSeek models available according to [DeepSeek API docs](https://api-docs.deepseek.com/quick_start/pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b7bd2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+_+] Saved batched 30 responses: deepseekEval/mdl-deepseek-chat_deepseek.xlsx\n",
      "EXECUTION TIME: 101.71 seconds\n",
      "________________________________________________________________________________\n",
      "[+_+] Saved batched 30 responses: deepseekEval/mdl-deepseek-reasoner_deepseek.xlsx\n",
      "EXECUTION TIME: 112.74 seconds\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in [\"deepseek-chat\", \"deepseek-reasoner\"]:\n",
    "    start = time.time()\n",
    "    run_batched_test_for_persona(\n",
    "        persona_text=sarah_persona_text,\n",
    "        train_df=sarah_train,\n",
    "        test_df=test_df,\n",
    "        out_path=f\"deepseekEval/mdl-{str(i)}.xlsx\",\n",
    "        provider=\"deepseek\",\n",
    "        deepseek_model=i\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f\"EXECUTION TIME: {end - start:.2f} seconds\")\n",
    "    print(\"_\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f9d28",
   "metadata": {},
   "source": [
    "### 6.2: Gemini Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cfcd3e",
   "metadata": {},
   "source": [
    "Testing the differnt Gemini models available for free tier according to [Gemini API docs](https://ai.google.dev/gemini-api/docs/models) (pro doesnt seem to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "345e9f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+_+] Saved batched 30 responses: geminiEval/mdl-gemini-2.5-flash-lite_gemini.xlsx\n",
      "EXECUTION TIME: 6.62 seconds\n",
      "________________________________________________________________________________\n",
      "[+_+] Saved batched 30 responses: geminiEval/mdl-gemini-2.5-flash_gemini.xlsx\n",
      "EXECUTION TIME: 25.71 seconds\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\"]:\n",
    "    start = time.time()\n",
    "    run_batched_test_for_persona(\n",
    "        persona_text=sarah_persona_text,\n",
    "        train_df=sarah_train,\n",
    "        test_df=test_df,\n",
    "        out_path=f\"geminiEval/mdl-{str(i)}.xlsx\",\n",
    "        provider=\"gemini\",\n",
    "        gemini_model=i\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f\"EXECUTION TIME: {end - start:.2f} seconds\")\n",
    "    print(\"_\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc696827",
   "metadata": {},
   "source": [
    "# Step 7: Quantitative Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81b06f",
   "metadata": {},
   "source": [
    "### Step 7.1: Helpers (embeddings + readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim whitespace and coerce None to empty string\n",
    "def normalize_text(s: str) -> str:\n",
    "    return (s or \"\").strip()\n",
    "\n",
    "# Count alphabetic words (handles simple apostrophes)\n",
    "def word_count(t: str) -> int:\n",
    "    return len(re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", t or \"\"))\n",
    "\n",
    "# Flesch Reading Ease via textstat (fallback to 0.0 on error)\n",
    "def readability_score(t: str) -> float:\n",
    "    try:\n",
    "        return float(textstat.flesch_reading_ease(t or \"\"))\n",
    "    except Exception:\n",
    "        return print(\"Could not compute readability score\")\n",
    "\n",
    "# MiniLM sentence embeddings (normalized so cosine == dot)\n",
    "_emb = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Cached normalized embedding for a given text\n",
    "@lru_cache(maxsize=4096)\n",
    "def emb(text: str) -> np.ndarray:\n",
    "    return _emb.encode(text or \"\", normalize_embeddings=True)\n",
    "\n",
    "# Cosine similarity with defensive normalization\n",
    "def dot_cos(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(u) * np.linalg.norm(v)) + 1e-8\n",
    "    return float(np.dot(u, v) / denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc3c7d",
   "metadata": {},
   "source": [
    "### Step 7.2: Metrics & Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute (mean_similarity, diversity) across options; diversity = 1 - mean_similarity\n",
    "def option_diversity(options: list[str]) -> Tuple[float, float]:\n",
    "    vecs = [emb(o) for o in options if normalize_text(o)]\n",
    "    if len(vecs) < 2:\n",
    "        return 1.0, 0.0\n",
    "    sims = [dot_cos(vecs[i], vecs[j]) for i in range(len(vecs)) for j in range(i+1, len(vecs))]\n",
    "    mean_sim = float(sum(sims) / len(sims))\n",
    "    return mean_sim, float(1.0 - mean_sim)\n",
    "\n",
    "# evaluate a single row: readability, average words, and diversity\n",
    "def evaluate_row(row: pd.Series) -> dict:\n",
    "    options = [\n",
    "        normalize_text(row.get(\"Option_1\", \"\")),\n",
    "        normalize_text(row.get(\"Option_2\", \"\")),\n",
    "        normalize_text(row.get(\"Option_3\", \"\")),\n",
    "    ]\n",
    "\n",
    "    non_empty = [o for o in options if o]\n",
    "    read = float(np.mean([readability_score(o) for o in non_empty] or [0.0]))  # mean Flesch\n",
    "    avg_words = float(np.mean([word_count(o) for o in non_empty] or [0.0]))    # mean word count\n",
    "    mean_sim, diversity = option_diversity(options)                            # embedding-based variety\n",
    "\n",
    "    return {\n",
    "        \"flesch_reading_ease\": read,\n",
    "        \"avg_words\": avg_words,\n",
    "        \"diversity\": diversity,                 # keep last in printed table\n",
    "        \"mean_option_similarity\": mean_sim,     # diagnostic only\n",
    "    }\n",
    "\n",
    "# parse model/provider from stem \"mdl-{model}_{provider}\"\n",
    "def _meta_from_stem(stem: str) -> dict:\n",
    "    m = re.match(r\"^mdl-(.+)_(gemini|deepseek)$\", stem, re.I)\n",
    "    return {\n",
    "        \"model\": (m.group(1) if m else \"unknown\"),\n",
    "        \"provider\": (m.group(2).lower() if m else \"unknown\"),\n",
    "    }\n",
    "\n",
    "# print compact per-model mean metrics\n",
    "def _print_model_summary(provider: str, model: str, df: pd.DataFrame):\n",
    "    means = df.mean(numeric_only=True)\n",
    "    view = pd.DataFrame([\n",
    "        (\"flesch_reading_ease\",  means.get(\"flesch_reading_ease\", np.nan)),\n",
    "        (\"avg_words\",            means.get(\"avg_words\", np.nan)),\n",
    "        (\"diversity\",            means.get(\"diversity\", np.nan)),\n",
    "    ], columns=[\"metric\", \"value\"])\n",
    "    print(f\"\\n=== {provider.upper()} | {model} ===\")\n",
    "    print(view.to_string(index=False))\n",
    "\n",
    "# scan folders, evaluate files, print per-model summaries, return per-model means\n",
    "def run_eval_from_folders(\n",
    "    folders: tuple[str, ...] = (\"geminiEval\", \"deepseekEval\"),\n",
    "    pattern: str = \"mdl-*.xlsx\",\n",
    ") -> pd.DataFrame:\n",
    "    paths = [p for folder in folders for p in sorted(Path(folder).glob(pattern))]  # collect files\n",
    "    if not paths:\n",
    "        print(\"No files found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    recs = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_excel(p)\n",
    "            if not {\"Setting\", \"Question\", \"Option_1\", \"Option_2\", \"Option_3\"}.issubset(df.columns):  # schema check\n",
    "                continue\n",
    "            rows = [evaluate_row(r) for _, r in df.iterrows()]  # row metrics\n",
    "            m = _meta_from_stem(p.stem)  # model/provider\n",
    "            out = pd.DataFrame(rows)\n",
    "            out[\"provider\"] = m[\"provider\"]\n",
    "            out[\"model\"] = m[\"model\"]\n",
    "            recs.append(out)\n",
    "        except Exception as e:\n",
    "            print(f\"!! Skipping one file: {e}\")  # keep going\n",
    "\n",
    "    if not recs:\n",
    "        print(\"No successful evaluations.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_df = pd.concat(recs, ignore_index=True)  # stack all rows\n",
    "\n",
    "    # Print provider/model summaries (no filenames)\n",
    "    for (prov, mdl), grp in all_df.groupby([\"provider\", \"model\"]):  # print summaries\n",
    "        _print_model_summary(prov, mdl, grp)\n",
    "\n",
    "    # Return means per provider/model for downstream use\n",
    "    return (\n",
    "        all_df.groupby([\"provider\", \"model\"], as_index=False)  # per-model means\n",
    "              .mean(numeric_only=True)\n",
    "              .sort_values([\"provider\", \"model\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040131f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEEPSEEK | deepseek-chat ===\n",
      "             metric     value\n",
      "flesch_reading_ease 64.365406\n",
      "          avg_words 11.688889\n",
      "          diversity       NaN\n",
      "\n",
      "=== DEEPSEEK | deepseek-reasoner ===\n",
      "             metric     value\n",
      "flesch_reading_ease 60.568871\n",
      "          avg_words 12.077778\n",
      "          diversity       NaN\n",
      "\n",
      "=== GEMINI | gemini-2.5-flash ===\n",
      "             metric     value\n",
      "flesch_reading_ease 69.449849\n",
      "          avg_words 10.677778\n",
      "          diversity       NaN\n",
      "\n",
      "=== GEMINI | gemini-2.5-flash-lite ===\n",
      "             metric     value\n",
      "flesch_reading_ease 73.851537\n",
      "          avg_words  9.066667\n",
      "          diversity       NaN\n"
     ]
    }
   ],
   "source": [
    "# Runt the quantitative evaluation across all interested models\n",
    "results = run_eval_from_folders(\n",
    "    folders=(\"geminiEval\", \"deepseekEval\"),\n",
    "    pattern=\"mdl-*.xlsx\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
